# langgraph_rag.py

from typing import Any, Dict
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver

from langchain_ollama import OllamaLLM
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma


# 1Ô∏è‚É£ Load college data
with open("college_details.txt", "r", encoding="utf-8") as f:
    college_text = f.read()

# 2Ô∏è‚É£ Initialize Hugging Face embeddings
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# 3Ô∏è‚É£ Setup or load ChromaDB
print("üì• Creating or loading Chroma database...")
db = Chroma.from_texts([college_text], embedding_model, persist_directory="./college_db")


# 4Ô∏è‚É£ Define nodes
def ensure_db_node(state: Dict[str, Any]) -> Dict[str, Any]:
    print("‚úÖ ChromaDB ready with embeddings.")
    state["db_ready"] = True
    return state


def retrieval_node(state: Dict[str, Any]) -> Dict[str, Any]:
    question = state["question"]
    print(f"üîç Performing semantic search for: {question}")
    docs = db.similarity_search(question)
    context = "\n".join([d.page_content for d in docs])
    state["context"] = context
    return state


def generator_node(state: Dict[str, Any]) -> Dict[str, Any]:
    model = OllamaLLM(model="gemma3-finetuned")
    question = state["question"]
    context = state["context"]

    prompt = f"Use the following college data to answer accurately:\n{context}\n\nQuestion: {question}"
    print("\nüß† Thinking...\n")
    answer = model.invoke(prompt)

    print("üí¨ Answer:\n")
    print(answer)
    state["answer"] = answer
    return state


# 5Ô∏è‚É£ Build LangGraph
graph = StateGraph(dict)
graph.add_node("ensure_db", ensure_db_node)
graph.add_node("retrieval", retrieval_node)
graph.add_node("generator", generator_node)

graph.add_edge(START, "ensure_db")
graph.add_edge("ensure_db", "retrieval")
graph.add_edge("retrieval", "generator")
graph.add_edge("generator", END)

memory = MemorySaver()
app = graph.compile(checkpointer=memory)


# 6Ô∏è‚É£ Run the graph
if __name__ == "__main__":
    question = input("ü§î Ask about your college: ")
    initial_state = {"question": question}

    # ‚úÖ Add this line ‚Äî gives LangGraph a thread/session ID
    config = {"configurable": {"thread_id": "college_thread"}}

    # ‚úÖ Pass config to the invoke method
    response = app.invoke(initial_state, config=config)

    print("\n===========================")
    print("‚úÖ Final Answer:")
    print(response["answer"])
    print("===========================")
